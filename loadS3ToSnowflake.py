from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from datetime import datetime, timedelta
import logging

# Airflow í™˜ê²½ ë³€ìˆ˜ì—ì„œ Snowflake & S3 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
SNOWFLAKE_CONN_ID = "snowflake_conn"
STAGE_NAME = "S3_STAGE"
TABLE_NAME = "gyoung.kopis_performance_raw"

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2024, 6, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "load_s3_to_snowflake_period",
    default_args=default_args,
    schedule=None,  # ìˆ˜ë™ ì‹¤í–‰, í•„ìš”í•˜ë©´ ì¼ì • ì¶”ê°€ ê°€ëŠ¥
    catchup=False
)

# ğŸ“Œ Snowflake COPY ì‹¤í–‰ í•¨ìˆ˜
def load_s3_to_snowflake_period():
    snowflake_hook = SnowflakeHook(snowflake_conn_id=SNOWFLAKE_CONN_ID)

    copy_sql = f"""
        COPY INTO {TABLE_NAME} 
        FROM (
            SELECT 
                $1::INT, 
                $2::INT, 
                $3::INT, 
                $4::BIGINT, 
                $5::INT, 
                $6::INT, 
                TO_DATE($7, 'YYYYMMDD'),  -- âœ” prfdtë¥¼ DATE íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                $8::INT
            FROM @{STAGE_NAME}
        )
        FILE_FORMAT = (TYPE = CSV, FIELD_OPTIONALLY_ENCLOSED_BY='"', SKIP_HEADER=1);
    """

    logging.info(f"Executing COPY command: {copy_sql}")

    try:
        snowflake_hook.run(copy_sql)
        logging.info(f"âœ… S3ì—ì„œ Snowflakeë¡œ ë°ì´í„° ì ì¬ ì™„ë£Œ.")
    except Exception as e:
        logging.error(f"âŒ Snowflake ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {e}")
        raise

# S3 â†’ Snowflake ì ì¬ íƒœìŠ¤í¬
load_to_snowflake = PythonOperator(
    task_id="load_s3_to_snowflake",
    python_callable=load_s3_to_snowflake_period,
    dag=dag
)

load_to_snowflake