from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook

from datetime import datetime, timedelta
import logging

# Airflow í™˜ê²½ ë³€ìˆ˜ì—ì„œ Snowflake & S3 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
SNOWFLAKE_CONN_ID = "snowflake_conn"
STAGE_NAME = "S3_STAGE"
TABLE_NAME = "gyoung.kopis_performance_raw"  # JSON ì ì¬ìš© í…Œì´ë¸”

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2024, 6, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "s3_to_snowflake_kopis",
    default_args=default_args,
    schedule=None,  # ìˆ˜ë™ ì‹¤í–‰, í•„ìš”í•˜ë©´ ì¼ì • ì¶”ê°€ ê°€ëŠ¥
    catchup=False
)

# ğŸ“Œ Snowflake COPY ì‹¤í–‰ í•¨ìˆ˜
def load_s3_to_snowflake():
    snowflake_hook = SnowflakeHook(snowflake_conn_id=SNOWFLAKE_CONN_ID)
    
    copy_sql = f"""
        COPY INTO {TABLE_NAME}
        FROM @{STAGE_NAME}
        FILE_FORMAT = (TYPE = JSON);
    """

    logging.info(f"Executing COPY command: {copy_sql}")
    
    try:
        snowflake_hook.run(copy_sql)
        logging.info(f"âœ… S3ì—ì„œ Snowflakeë¡œ ë°ì´í„° ì ì¬ ì™„ë£Œ.")
    except Exception as e:
        logging.error(f"âŒ Snowflake ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {e}")
        raise

# S3 â†’ Snowflake ì ì¬ íƒœìŠ¤í¬
load_to_snowflake = PythonOperator(
    task_id="load_s3_to_snowflake",
    python_callable=load_s3_to_snowflake,
    dag=dag
)

load_to_snowflake